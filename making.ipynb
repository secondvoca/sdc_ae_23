{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### myae2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def order_dataset(default=False):\n",
    "    set_names = ['mnist', 'fashion-mnist']\n",
    "    if default:\n",
    "        idx_set = 0\n",
    "    else:\n",
    "        idx_set = int(input(str([f'{idx}: {n}' for idx, n in enumerate(set_names)])))\n",
    "    sets = [datasets.MNIST, datasets.FashionMNIST]\n",
    "    data = sets[idx_set](root='/home/secondvoca/sdc_ae_23/data', train=True, download=True, transform=ToTensor())\n",
    "    return data\n",
    "data = order_dataset()\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Keeper:\n",
    "    def prepare(self, less_than=10, batch_size=128, shuffle=True):\n",
    "        training_data = datasets.MNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=ToTensor(),\n",
    "        )\n",
    "\n",
    "        training_data.data = training_data.data[training_data.targets < less_than]\n",
    "        training_data.targets = training_data.targets[training_data.targets < less_than]\n",
    "\n",
    "        self.training_data = training_data.data / 255.0\n",
    "        self.training_targets = training_data.targets\n",
    "\n",
    "        self.training_data_length = len(training_data.data)\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            training_data, batch_size=batch_size, shuffle=shuffle\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_builder:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager:\n",
    "    \n",
    "\n",
    "    def set_model(self, encoder, decoder):\n",
    "        self.model = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"encoder\", encoder),\n",
    "                    (\"decoder\", decoder),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "    def get_cuda_device_or_cpu(self):\n",
    "        if torch.cuda.is_available():\n",
    "            cuda_count = torch.cuda.device_count()\n",
    "\n",
    "            no = 0\n",
    "            mem_available = 0\n",
    "\n",
    "            for i in range(cuda_count):\n",
    "                tmp_available = torch.cuda.mem_get_info(i)[0]\n",
    "                if mem_available < tmp_available:\n",
    "                    no = i\n",
    "                    mem_available = tmp_available\n",
    "            return f\"cuda:{no}\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    def save_current_model(self, name):\n",
    "        torch.save(\n",
    "            self.model,\n",
    "            f'./models/{datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")}_{name}.pt',\n",
    "        )\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model = torch.load(f\"./models/{name}.pt\")\n",
    "\n",
    "    def run(self, model, dataloader, optimizer, device, calc_loss):\n",
    "        hist = torch.zeros(len(dataloader))\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            x = x.view([-1, 28 * 28]).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            loss = calc_loss(model, x, y, F, device=device)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hist[batch] = loss.item()\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def train(self, calc_loss, epochs=5, need_latent_record=False, ):\n",
    "        try:\n",
    "            device = self.get_cuda_device_or_cpu()\n",
    "        except:\n",
    "            device = \"cpu\"\n",
    "        print(f\"Now, it is working on {device}.\")\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "\n",
    "        hist = torch.zeros(0)\n",
    "\n",
    "        for _ in tqdm(range(epochs)):\n",
    "            tmp = self.run(\n",
    "                self.model, self.train_dataloader, self.optimizer, device, calc_loss\n",
    "            )\n",
    "            hist = torch.cat([hist, tmp])\n",
    "\n",
    "        return hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
